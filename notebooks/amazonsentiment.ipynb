{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7837e3b",
   "metadata": {},
   "source": [
    "# **Sentiment Analysis from Scratch Using Logistic Regression**\n",
    "\n",
    "**Name:** Harshkumar Rana  \n",
    "**Roll Number:** 22BCP091  \n",
    "\n",
    "---\n",
    "\n",
    "## **Project Overview**\n",
    "\n",
    "This project implements a complete sentiment analysis pipeline from scratch using Python. The objective is to classify Amazon product reviews as *positive* or *negative* based on their textual content.  \n",
    "\n",
    "The workflow includes:\n",
    "- Loading and preprocessing the Amazon review dataset  \n",
    "- Constructing a vocabulary and generating TF-IDF-based text representations  \n",
    "- Implementing Logistic Regression training using batch gradient descent with L2 regularization  \n",
    "- Evaluating model performance using metrics such as accuracy, precision, recall, and F1-score  \n",
    "- Displaying a confusion matrix and identifying the most influential words contributing to predictions  \n",
    "\n",
    "All computations, including model training and evaluation, are implemented manually without using external machine learning or visualization libraries like Matplotlib. The results are displayed directly in text format for clarity and compliance with project requirements.\n",
    "\n",
    "---\n",
    "\n",
    "## **Dataset**\n",
    "\n",
    "The dataset contains Amazon product reviews with numerical ratings. Reviews rated **4 or 5** are considered *positive*, while those rated **1 or 2** are considered *negative*. Neutral reviews (rating 3) are excluded to maintain a balanced binary classification. A subset of data is used to ensure efficient training on available hardware.\n",
    "\n",
    "---\n",
    "\n",
    "## **Model and Training**\n",
    "\n",
    "A custom Logistic Regression model was implemented entirely from first principles. The model includes manual implementations of the sigmoid activation function, dot product computation, binary cross-entropy loss, and parameter updates using batch gradient descent.  \n",
    "Regularization is applied to prevent overfitting, and the model is trained over multiple epochs for stable convergence.\n",
    "\n",
    "---\n",
    "\n",
    "## **Goals**\n",
    "\n",
    "- Develop a clear understanding of text feature extraction techniques such as **TF-IDF**  \n",
    "- Gain hands-on experience implementing **machine learning algorithms from scratch**  \n",
    "- Learn to evaluate classification performance using standard metrics  \n",
    "- Focus on algorithmic understanding rather than pre-built libraries or external visualizations  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "740f3b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import math\n",
    "import random\n",
    "import collections\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "# ---------- USER CONFIG ----------\n",
    "DATAPATH = \"../data/Reviews.csv\"  # change if your CSV is elsewhere\n",
    "MAXSAMPLESPERCLASS = 50000  # per class (reduce if memory/time is tight)\n",
    "VOCABSIZE = 5000  # top-k words to keep\n",
    "TESTRATIO = 0.2  # fraction for test set\n",
    "RANDOMSEED = 42\n",
    "LEARNINGRATE = 0.5\n",
    "EPOCHS = 30\n",
    "BATCHSIZE = 128\n",
    "L2REG = 1e-4\n",
    "# ---------------------------------\n",
    "random.seed(RANDOMSEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e4d884f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset (this may take a few seconds)...\n",
      "Total binary-labeled samples in file: 525814\n"
     ]
    }
   ],
   "source": [
    "def load_reviews(path: str):\n",
    "    data = []\n",
    "    with open(path, newline=\"\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for i, row in enumerate(reader):\n",
    "            if \"Text\" not in row or \"Score\" not in row:\n",
    "                continue\n",
    "            text = row[\"Text\"].strip()\n",
    "            try:\n",
    "                score = float(row[\"Score\"])\n",
    "            except:\n",
    "                continue\n",
    "            if score >= 4:\n",
    "                data.append((text, 1))\n",
    "            elif score <= 2:\n",
    "                data.append((text, 0))\n",
    "    return data\n",
    "\n",
    "\n",
    "print(\"Loading dataset (this may take a few seconds)...\")\n",
    "all_data = load_reviews(DATAPATH)\n",
    "print(\"Total binary-labeled samples in file:\", len(all_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ed9f423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After sampling balanced subset: 100000 - positives: 50000, negatives: 50000\n"
     ]
    }
   ],
   "source": [
    "def sample_balanced(data: List[Tuple[str, int]], max_per_class: int):\n",
    "    pos = [t for t in data if t[1] == 1]\n",
    "    neg = [t for t in data if t[1] == 0]\n",
    "    random.shuffle(pos)\n",
    "    random.shuffle(neg)\n",
    "    pos = pos[:max_per_class]\n",
    "    neg = neg[:max_per_class]\n",
    "    sampled = pos + neg\n",
    "    random.shuffle(sampled)\n",
    "    return sampled\n",
    "\n",
    "data = sample_balanced(all_data, MAXSAMPLESPERCLASS)\n",
    "print(f\"After sampling balanced subset: {len(data)} - positives: {sum(1 for _, l in data if l == 1)}, negatives: {sum(1 for _, l in data if l == 0)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d63c704d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loved', 'product', 'amazing', 'would', 'buy', 'again']\n"
     ]
    }
   ],
   "source": [
    "STOPWORDS = set(\n",
    "    [\n",
    "        \"the\",\n",
    "        \"and\",\n",
    "        \"a\",\n",
    "        \"an\",\n",
    "        \"is\",\n",
    "        \"it\",\n",
    "        \"this\",\n",
    "        \"that\",\n",
    "        \"to\",\n",
    "        \"for\",\n",
    "        \"of\",\n",
    "        \"i\",\n",
    "        \"you\",\n",
    "        \"was\",\n",
    "        \"with\",\n",
    "        \"on\",\n",
    "        \"in\",\n",
    "        \"my\",\n",
    "        \"we\",\n",
    "        \"they\",\n",
    "        \"he\",\n",
    "        \"she\",\n",
    "        \"but\",\n",
    "        \"not\",\n",
    "        \"are\",\n",
    "        \"as\",\n",
    "        \"have\",\n",
    "        \"had\",\n",
    "        \"be\",\n",
    "        \"at\",\n",
    "        \"from\",\n",
    "        \"so\",\n",
    "        \"if\",\n",
    "        \"or\",\n",
    "        \"its\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def preprocess_text(text: str) -> list:\n",
    "    # 1. Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # 2. Remove punctuation (anything not a-z, A-Z, 0-9, or whitespace)\n",
    "    allowed_chars = \"abcdefghijklmnopqrstuvwxyz0123456789 \"\n",
    "    cleaned = []\n",
    "    for ch in text:\n",
    "        if ch in allowed_chars:\n",
    "            cleaned.append(ch)\n",
    "        else:\n",
    "            cleaned.append(\" \")\n",
    "    cleaned_text = \"\".join(cleaned)\n",
    "\n",
    "    # 3. Replace multiple spaces with one space (manual multi-space removal)\n",
    "    final_text = []\n",
    "    prev_char = None\n",
    "    for ch in cleaned_text:\n",
    "        if ch == \" \" and prev_char == \" \":\n",
    "            continue\n",
    "        final_text.append(ch)\n",
    "        prev_char = ch\n",
    "    final_text_str = \"\".join(final_text).strip()\n",
    "\n",
    "    # 4. Split into tokens\n",
    "    tokens = final_text_str.split(\" \")\n",
    "\n",
    "    # 5. Filter out stopwords, tokens of length <= 1, and tokens that are all digits\n",
    "    filtered_tokens = []\n",
    "    for token in tokens:\n",
    "        if len(token) > 1 and token not in STOPWORDS and not token.isdigit():\n",
    "            filtered_tokens.append(token)\n",
    "\n",
    "    return filtered_tokens\n",
    "\n",
    "\n",
    "# Quick test\n",
    "print(\n",
    "    preprocess_text(\"I loved this product! It's amazing, would buy again. 1010\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9869aa3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 5000\n",
      "Sample vocab items: [('like', 0), ('br', 1), ('these', 2), ('good', 3), ('taste', 4), ('just', 5), ('one', 6), ('product', 7), ('very', 8), ('all', 9)]\n"
     ]
    }
   ],
   "source": [
    "def build_vocab(doc_tokens: List[List[str]], vocab_size: int):\n",
    "    df = collections.Counter()\n",
    "    for tokens in doc_tokens:\n",
    "        unique = set(tokens)\n",
    "        for t in unique:\n",
    "            df[t] += 1\n",
    "    most_common = df.most_common(vocab_size)\n",
    "    vocab = {w: idx for idx, (w, _) in enumerate(most_common)}\n",
    "    doc_freqs = [count for _, count in most_common]\n",
    "    return vocab, doc_freqs\n",
    "\n",
    "\n",
    "doc_tokens = [preprocess_text(text) for text, _ in data]\n",
    "vocab, doc_freqs = build_vocab(doc_tokens, VOCABSIZE)\n",
    "print(\"Vocab size:\", len(vocab))\n",
    "print(\"Sample vocab items:\", list(vocab.items())[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2887dbdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Distribution:\n",
      "Positive:  50000 (50.00%)\n",
      "Negative:  50000 (50.00%)\n",
      "\n",
      "Top 10 Tokens Frequency:\n",
      "br             | ################################################## (117619)\n",
      "like           | #################### (47937)\n",
      "these          | ################ (39679)\n",
      "them           | ############### (35896)\n",
      "product        | ############## (34377)\n",
      "taste          | ############# (32932)\n",
      "one            | ############# (32289)\n",
      "just           | ############# (31539)\n",
      "good           | ############# (31119)\n",
      "can            | ############ (29914)\n"
     ]
    }
   ],
   "source": [
    "# Visualize class distribution (positive vs negative samples)\n",
    "def print_class_distribution(data):\n",
    "    pos = 0\n",
    "    neg = 0\n",
    "    for _, label in data:\n",
    "        if label == 1:\n",
    "            pos += 1\n",
    "        else:\n",
    "            neg += 1\n",
    "\n",
    "    total = pos + neg\n",
    "    print(\"Class Distribution:\")\n",
    "    print(\"Positive: \", pos, \"({:.2f}%)\".format((pos / total) * 100))\n",
    "    print(\"Negative: \", neg, \"({:.2f}%)\".format((neg / total) * 100))\n",
    "\n",
    "\n",
    "# Simple text bar chart for token frequencies\n",
    "def print_simple_bar_chart_no_lib(values, labels):\n",
    "    max_val = 0\n",
    "    for v in values:\n",
    "        if v > max_val:\n",
    "            max_val = v\n",
    "\n",
    "    max_width = 50\n",
    "    for i in range(len(values)):\n",
    "        val = values[i]\n",
    "        label = labels[i]\n",
    "        bar_length = (val * max_width) // max_val\n",
    "        label_str = label + (\" \" * (15 - len(label)))\n",
    "        bar_str = \"\"\n",
    "        for _ in range(bar_length):\n",
    "            bar_str += \"#\"\n",
    "        print(label_str + \"| \" + bar_str + \" (\" + str(val) + \")\")\n",
    "\n",
    "\n",
    "# Example usage to show top 10 tokens frequency in your vocab\n",
    "def visualize_top_tokens_frequency(vocab, doc_tokens):\n",
    "    token_counts = {}\n",
    "    for tokens in doc_tokens:\n",
    "        for t in tokens:\n",
    "            token_counts[t] = token_counts.get(t, 0) + 1\n",
    "\n",
    "    # Sort tokens by frequency\n",
    "    sorted_tokens = sorted(token_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    top_tokens = sorted_tokens[:10]\n",
    "\n",
    "    labels = [t[0] for t in top_tokens]\n",
    "    values = [t[1] for t in top_tokens]\n",
    "\n",
    "    print(\"\\nTop 10 Tokens Frequency:\")\n",
    "    print_simple_bar_chart_no_lib(values, labels)\n",
    "\n",
    "\n",
    "# Call functions with your data variables\n",
    "print_class_distribution(data)\n",
    "visualize_top_tokens_frequency(vocab, doc_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21c8253b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built feature matrix N = 100000 features per sample = 5000\n"
     ]
    }
   ],
   "source": [
    "def compute_idf(doc_freqs: List[int], num_docs: int):\n",
    "    idf = []\n",
    "    for df in doc_freqs:\n",
    "        idf.append(math.log(1 + num_docs / (1 + df)))\n",
    "    return idf\n",
    "\n",
    "\n",
    "IDF = compute_idf(doc_freqs, len(doc_tokens))\n",
    "\n",
    "\n",
    "def vectorize_tfidf(tokens: List[str], vocab: Dict[str, int], idf: List[float]):\n",
    "    tf = [0.0] * len(vocab)\n",
    "    for t in tokens:\n",
    "        if t in vocab:\n",
    "            tf[vocab[t]] += 1.0\n",
    "    total = sum(tf)\n",
    "    if total == 0:\n",
    "        return tf\n",
    "    for i in range(len(tf)):\n",
    "        tf[i] = tf[i] / total * idf[i]\n",
    "    return tf\n",
    "\n",
    "\n",
    "X = [vectorize_tfidf(tokens, vocab, IDF) for tokens in doc_tokens]\n",
    "y = [label for _, label in data]\n",
    "print(\"Built feature matrix N =\", len(X), \"features per sample =\", len(X[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b296b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train = 80000 Test = 20000\n"
     ]
    }
   ],
   "source": [
    "def train_test_split(X, y, test_ratio=TESTRATIO):\n",
    "    idxs = list(range(len(X)))\n",
    "    random.shuffle(idxs)\n",
    "    split = int(len(idxs) * (1 - test_ratio))\n",
    "    train_idx = idxs[:split]\n",
    "    test_idx = idxs[split:]\n",
    "    X_train = [X[i] for i in train_idx]\n",
    "    y_train = [y[i] for i in train_idx]\n",
    "    X_test = [X[i] for i in test_idx]\n",
    "    y_test = [y[i] for i in test_idx]\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "print(\"Train =\", len(X_train), \"Test =\", len(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d007e49b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 - loss 0.6431\n",
      "Epoch 2/30 - loss 0.6062\n",
      "Epoch 3/30 - loss 0.5788\n",
      "Epoch 4/30 - loss 0.5579\n",
      "Epoch 5/30 - loss 0.5418\n",
      "Epoch 6/30 - loss 0.5294\n",
      "Epoch 7/30 - loss 0.5190\n",
      "Epoch 8/30 - loss 0.5110\n",
      "Epoch 9/30 - loss 0.5045\n",
      "Epoch 10/30 - loss 0.4989\n",
      "Epoch 11/30 - loss 0.4944\n",
      "Epoch 12/30 - loss 0.4907\n",
      "Epoch 13/30 - loss 0.4873\n",
      "Epoch 14/30 - loss 0.4846\n",
      "Epoch 15/30 - loss 0.4824\n",
      "Epoch 16/30 - loss 0.4810\n",
      "Epoch 17/30 - loss 0.4794\n",
      "Epoch 18/30 - loss 0.4774\n",
      "Epoch 19/30 - loss 0.4762\n",
      "Epoch 20/30 - loss 0.4751\n",
      "Epoch 21/30 - loss 0.4744\n",
      "Epoch 22/30 - loss 0.4743\n",
      "Epoch 23/30 - loss 0.4728\n",
      "Epoch 24/30 - loss 0.4723\n",
      "Epoch 25/30 - loss 0.4717\n",
      "Epoch 26/30 - loss 0.4712\n",
      "Epoch 27/30 - loss 0.4708\n",
      "Epoch 28/30 - loss 0.4704\n",
      "Epoch 29/30 - loss 0.4701\n",
      "Epoch 30/30 - loss 0.4698\n"
     ]
    }
   ],
   "source": [
    "def dot(a, b):\n",
    "    return sum(ai * bi for ai, bi in zip(a, b))\n",
    "\n",
    "\n",
    "def sigmoid(z):\n",
    "    if z >= 0:\n",
    "        ez = math.exp(-z)\n",
    "        return 1.0 / (1.0 + ez)\n",
    "    else:\n",
    "        ez = math.exp(z)\n",
    "        return ez / (1.0 + ez)\n",
    "\n",
    "\n",
    "class LogisticRegressionScratch:\n",
    "    def __init__(self, nfeatures, lr=0.1, l2=0.0):\n",
    "        self.w = [random.uniform(-0.01, 0.01) for _ in range(nfeatures)]\n",
    "        self.b = 0.0\n",
    "        self.lr = lr\n",
    "        self.l2 = l2\n",
    "\n",
    "    def predict_proba_single(self, x):\n",
    "        z = dot(self.w, x) + self.b\n",
    "        return sigmoid(z)\n",
    "\n",
    "    def predict_single(self, x, threshold=0.5):\n",
    "        return 1 if self.predict_proba_single(x) >= threshold else 0\n",
    "\n",
    "    def compute_loss(self, X, y):\n",
    "        total = 0.0\n",
    "        for xi, yi in zip(X, y):\n",
    "            p = max(1e-12, min(1 - 1e-12, self.predict_proba_single(xi)))\n",
    "            total += -yi * math.log(p) - (1 - yi) * math.log(1 - p)\n",
    "        avg = total / len(X)\n",
    "        l2_term = 0.5 * self.l2 * sum(w * w for w in self.w)\n",
    "        return avg + l2_term\n",
    "\n",
    "    def fit(self, X, y, epochs=10, batch_size=32, verbose=True):\n",
    "        n = len(X)\n",
    "        loss_history = []\n",
    "        for epoch in range(epochs):\n",
    "            idxs = list(range(n))\n",
    "            random.shuffle(idxs)\n",
    "            for start in range(0, n, batch_size):\n",
    "                end = min(start + batch_size, n)\n",
    "                batch_idxs = idxs[start:end]\n",
    "                grad_w = [0.0] * len(self.w)\n",
    "                grad_b = 0.0\n",
    "                for i in batch_idxs:\n",
    "                    xi = X[i]\n",
    "                    yi = y[i]\n",
    "                    pi = self.predict_proba_single(xi)\n",
    "                    err = pi - yi\n",
    "                    for j in range(len(self.w)):\n",
    "                        grad_w[j] += err * xi[j]\n",
    "                    grad_b += err\n",
    "                batch_len = len(batch_idxs)\n",
    "                for j in range(len(self.w)):\n",
    "                    grad = grad_w[j] / batch_len + self.l2 * self.w[j]\n",
    "                    self.w[j] -= self.lr * grad\n",
    "                self.b -= self.lr * grad_b / batch_len\n",
    "            loss = self.compute_loss(X, y)\n",
    "            loss_history.append(loss)\n",
    "            if verbose:\n",
    "                print(f\"Epoch {epoch + 1}/{epochs} - loss {loss:.4f}\")\n",
    "        return loss_history\n",
    "\n",
    "\n",
    "model = LogisticRegressionScratch(nfeatures=len(vocab), lr=LEARNINGRATE, l2=L2REG)\n",
    "losshist = model.fit(X_train, y_train, epochs=EPOCHS, batch_size=BATCHSIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d92abf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Curve Over Epochs:\n",
      "Epoch   1: ████████████████████████████████████████ 0.6431\n",
      "Epoch   2: █████████████████████████████████████    0.6062\n",
      "Epoch   3: ███████████████████████████████████      0.5788\n",
      "Epoch   4: ██████████████████████████████████       0.5579\n",
      "Epoch   5: █████████████████████████████████        0.5418\n",
      "Epoch   6: ████████████████████████████████         0.5294\n",
      "Epoch   7: ████████████████████████████████         0.5190\n",
      "Epoch   8: ███████████████████████████████          0.5110\n",
      "Epoch   9: ███████████████████████████████          0.5045\n",
      "Epoch  10: ███████████████████████████████          0.4989\n",
      "Epoch  11: ██████████████████████████████           0.4944\n",
      "Epoch  12: ██████████████████████████████           0.4907\n",
      "Epoch  13: ██████████████████████████████           0.4873\n",
      "Epoch  14: ██████████████████████████████           0.4846\n",
      "Epoch  15: ██████████████████████████████           0.4824\n",
      "Epoch  16: █████████████████████████████            0.4810\n",
      "Epoch  17: █████████████████████████████            0.4794\n",
      "Epoch  18: █████████████████████████████            0.4774\n",
      "Epoch  19: █████████████████████████████            0.4762\n",
      "Epoch  20: █████████████████████████████            0.4751\n",
      "Epoch  21: █████████████████████████████            0.4744\n",
      "Epoch  22: █████████████████████████████            0.4743\n",
      "Epoch  23: █████████████████████████████            0.4728\n",
      "Epoch  24: █████████████████████████████            0.4723\n",
      "Epoch  25: █████████████████████████████            0.4717\n",
      "Epoch  26: █████████████████████████████            0.4712\n",
      "Epoch  27: █████████████████████████████            0.4708\n",
      "Epoch  28: █████████████████████████████            0.4704\n",
      "Epoch  29: █████████████████████████████            0.4701\n",
      "Epoch  30: █████████████████████████████            0.4698\n"
     ]
    }
   ],
   "source": [
    "def ascii_loss_curve(loss_history):\n",
    "    max_len = 40  # Maximum width of the plotted bar\n",
    "    if not loss_history or len(loss_history) == 0:\n",
    "        print(\"No loss history available to plot.\")\n",
    "        return\n",
    "    max_loss = max(loss_history)\n",
    "    print(\"Loss Curve Over Epochs:\")\n",
    "    for i, loss in enumerate(loss_history):\n",
    "        bar_len = int((loss / max_loss) * max_len) if max_loss > 0 else 0\n",
    "        bar = \"█\" * bar_len\n",
    "        print(f\"Epoch {i + 1:3}: {bar:<40} {loss:.4f}\")\n",
    "\n",
    "\n",
    "ascii_loss_curve(losshist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3a0a044f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def printmetrics(metrics, datasetname):\n",
    "    print(f\"\\n---- {datasetname} Metrics ----\")\n",
    "    print(f\"Accuracy:  {metrics['accuracy']:.4f}\")\n",
    "    print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"Recall:    {metrics['recall']:.4f}\")\n",
    "    print(f\"F1-score:  {metrics['f1']:.4f}\")\n",
    "    print(\n",
    "        f\"TP: {metrics['TP']}, FP: {metrics['FP']}, FN: {metrics['FN']}, TN: {metrics['TN']}\"\n",
    "    )\n",
    "    print(\"---------------------------\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "11a9ca56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---- Train Dataset Metrics ----\n",
      "Accuracy:  0.8725\n",
      "Precision: 0.8831\n",
      "Recall:    0.8590\n",
      "F1-score:  0.8709\n",
      "TP: 34403, FP: 4554, FN: 5645, TN: 35398\n",
      "---------------------------\n",
      "\n",
      "\n",
      "---- Test Dataset Metrics ----\n",
      "Accuracy:  0.8647\n",
      "Precision: 0.8712\n",
      "Recall:    0.8543\n",
      "F1-score:  0.8627\n",
      "TP: 8502, FP: 1257, FN: 1450, TN: 8791\n",
      "---------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def calculate_metrics(y_true, y_pred):\n",
    "    TP = sum((yt == 1 and yp == 1) for yt, yp in zip(y_true, y_pred))\n",
    "    TN = sum((yt == 0 and yp == 0) for yt, yp in zip(y_true, y_pred))\n",
    "    FP = sum((yt == 0 and yp == 1) for yt, yp in zip(y_true, y_pred))\n",
    "    FN = sum((yt == 1 and yp == 0) for yt, yp in zip(y_true, y_pred))\n",
    "\n",
    "    accuracy = (TP + TN) / len(y_true)\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0.0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0.0\n",
    "    f1_score = (\n",
    "        (2 * precision * recall) / (precision + recall)\n",
    "        if (precision + recall) > 0\n",
    "        else 0.0\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1_score,\n",
    "        \"TP\": TP,\n",
    "        \"FP\": FP,\n",
    "        \"FN\": FN,\n",
    "        \"TN\": TN,\n",
    "    }\n",
    "\n",
    "\n",
    "# Use your model instance to predict each sample's label:\n",
    "train_predictions = [model.predict_single(x) for x in X_train]\n",
    "test_predictions = [model.predict_single(x) for x in X_test]\n",
    "\n",
    "# Calculate metrics for train and test\n",
    "trainmetrics = calculate_metrics(y_train, train_predictions)\n",
    "testmetrics = calculate_metrics(y_test, test_predictions)\n",
    "\n",
    "# Now use your existing printmetrics(metrics, datasetname) function to display:\n",
    "printmetrics(trainmetrics, \"Train Dataset\")\n",
    "printmetrics(testmetrics, \"Test Dataset\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "61096ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top positive-weight tokens (indicative of positive class):\n",
      "great        8.4334\n",
      "best         5.6148\n",
      "love         5.4053\n",
      "delicious    4.8593\n",
      "good         4.3095\n",
      "perfect      4.2156\n",
      "loves        4.0710\n",
      "excellent    3.4111\n",
      "favorite     3.2869\n",
      "nice         3.2408\n",
      "easy         3.1342\n",
      "wonderful    3.0620\n",
      "highly       3.0052\n",
      "find         2.6271\n",
      "well         2.4817\n",
      "tasty        2.3941\n",
      "use          2.3780\n",
      "happy        2.3135\n",
      "always       2.3013\n",
      "snack        2.2816\n",
      "little       2.2775\n",
      "stores       2.2549\n",
      "day          2.2130\n",
      "smooth       2.1348\n",
      "amazing      2.1345\n",
      "Top negative-weight tokens (indicative of negative class):\n",
      "off          -2.0142\n",
      "china        -2.0708\n",
      "should       -2.1208\n",
      "even         -2.1355\n",
      "disappointing -2.1536\n",
      "maybe        -2.2702\n",
      "away         -2.3625\n",
      "unfortunately -2.4255\n",
      "didn         -2.4535\n",
      "weak         -2.4718\n",
      "stale        -2.4748\n",
      "return       -2.5254\n",
      "taste        -2.6373\n",
      "waste        -2.6821\n",
      "would        -2.7492\n",
      "terrible     -2.7567\n",
      "awful        -2.7631\n",
      "horrible     -2.8257\n",
      "did          -2.9202\n",
      "worst        -2.9332\n",
      "were         -3.0050\n",
      "thought      -3.1963\n",
      "money        -3.2110\n",
      "bad          -3.5079\n",
      "disappointed -4.4780\n"
     ]
    }
   ],
   "source": [
    "def top_features_by_weight(w: List[float], vocab: Dict[str, int], topk=25):\n",
    "    inv = {idx: word for word, idx in vocab.items()}\n",
    "    pairs = [(inv[i], wi) for i, wi in enumerate(w) if i in inv]\n",
    "    pairs.sort(key=lambda x: x[1], reverse=True)\n",
    "    top_pos = pairs[:topk]\n",
    "    top_neg = pairs[-topk:]\n",
    "    return top_pos, top_neg\n",
    "\n",
    "top_pos, top_neg = top_features_by_weight(model.w, vocab, topk=25)\n",
    "print(\"Top positive-weight tokens (indicative of positive class):\")\n",
    "for tok, wt in top_pos:\n",
    "    print(f\"{tok: <12} {wt:.4f}\")\n",
    "print(\"Top negative-weight tokens (indicative of negative class):\")\n",
    "for tok, wt in top_neg:\n",
    "    print(f\"{tok: <12} {wt:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e3cb6fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to logreg_model_v1.txt\n"
     ]
    }
   ],
   "source": [
    "def save_model_weights(path: str, model, vocab, idf):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"BINARYLOGREGMODELV1\\n\")\n",
    "        f.write(f\"strlenmodel.w {len(model.w)}\\n\")\n",
    "        f.write(f\"bias {model.b}\\n\")\n",
    "        f.write(\"weights\\n\")\n",
    "        for w in model.w:\n",
    "            f.write(f\"{w}\\n\")\n",
    "        f.write(\"vocab\\n\")\n",
    "        for word, idx in vocab.items():\n",
    "            f.write(f\"{word} {idx}\\n\")\n",
    "        f.write(\"idf\\n\")\n",
    "        for v in idf:\n",
    "            f.write(f\"{v}\\n\")\n",
    "\n",
    "\n",
    "save_model_weights(\"logreg_model_v1.txt\", model, vocab, IDF)\n",
    "print(\"Model saved to logreg_model_v1.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6353b2",
   "metadata": {},
   "source": [
    "# **Conclusion:**\n",
    "\n",
    "In this project, I developed a custom sentiment analysis model from scratch using Logistic Regression to classify Amazon product reviews as positive or negative. The implementation covered every stage of the machine learning pipeline from text preprocessing, tokenization, and feature vectorization to gradient-based optimization and evaluation. Without relying on pre-built ML libraries, the model was able to learn meaningful patterns from review data and achieve consistent accuracy across the dataset. Through this process, I gained a deeper understanding of how Logistic Regression works internally, including the impact of parameters like learning rate, regularization, and loss convergence. The visualization of token frequencies and class distribution helped interpret dataset balance and common word patterns. Overall, this project strengthened my understanding of core machine learning concepts and demonstrated how custom-built models can effectively solve real-world text classification problems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
